{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT Notebook\n",
    "\n",
    "This notebook assumes that you have finished finetuning the language model using the LM training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from finetuning import one_cycle\n",
    "from utils import produce_dataloaders, count_parameters\n",
    "from layers import AWDLSTMEncoder, ConcatPoolingDecoder, RNNClassifier\n",
    "from transformers import WarmupLinearSchedule\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);\n",
    "torch.cuda.manual_seed(42);\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and split them into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/imdb/clas_data/train.csv').sample(frac=1, random_state=42)\n",
    "text, sentiment = list(df['text']), list(df['sentiment'])\n",
    "\n",
    "tr_sz = int(len(text) * 0.7)\n",
    "\n",
    "X_train, y_train = text[:tr_sz], sentiment[:tr_sz]\n",
    "X_val, y_val = text[tr_sz:], sentiment[tr_sz:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 17500\n",
      "Validation Set: 7500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set: {}\\nValidation Set: {}\".format(len(X_train), len(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to tokenize our dataset. We use spacy for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en = spacy.load('en')\n",
    "\n",
    "def tokenize(t):\n",
    "    return [str(token) for token in en(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line will take a while.  We'll save it so we can just load the tokenized data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = [tokenize(t) for t in tqdm(X_train)]\n",
    "#X_val = [tokenize(t) for t in tqdm(X_val)]\n",
    "\n",
    "#with open('../data/imdb/clas_data/cache.pth', 'wb') as f:\n",
    "#    torch.save([X_train, X_val], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/imdb/clas_data/cache.pth', 'rb') as f:\n",
    "    X_train, X_val = torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll delimit the data to a maximum sequence length and pad shorter sequences. We also opt to drop the last batch which has an irregular batch size.\n",
    "\n",
    "In this step, we load the vocabulary of the finetuned language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17500/17500 [00:01<00:00, 10262.67it/s]\n",
      "100%|██████████| 7500/7500 [00:00<00:00, 12178.30it/s]\n"
     ]
    }
   ],
   "source": [
    "msl = 512\n",
    "bs = 64\n",
    "\n",
    "# Load the vocabulary\n",
    "with open('../data/pretrained_wt103/vocab.pth', 'rb') as f:\n",
    "    word2idx, idx2word = torch.load(f)\n",
    "vocab_set = set(idx2word)\n",
    "\n",
    "# Produce dataloaders\n",
    "train_loader, val_loader = produce_dataloaders(X_train, y_train, X_val, y_val, \n",
    "                                               word2idx, vocab_set, msl, bs, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AWDLSTMEncoder(vocab_sz=len(idx2word), emb_dim=400, hidden_dim=1152, num_layers=3)\n",
    "decoder = ConcatPoolingDecoder(hidden_dim=400, bneck_dim=50, out_dim=2)\n",
    "model = RNNClassifier(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/imdb/lm_data/imdb_finetuned.pth', 'rb') as f:\n",
    "    inc = model.load_state_dict(torch.load(f), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup our optimizers. Note that unlike the original paper, we use linear warmup scheduling instead of slanted triangular learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_groups = [{'name': '0', 'params': []}, {'name': '1', 'params': []}]\n",
    "for n, p in model.named_parameters():\n",
    "    if 'rnn' in n:\n",
    "        p_groups[1]['params'].append(p)\n",
    "    else:\n",
    "        p_groups[0]['params'].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(p_groups, lr=1e-2)\n",
    "\n",
    "epochs = 5\n",
    "steps = len(train_loader) * epochs\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=int(steps * 0.1), t_total=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And gradually unfreeze while finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [03:22<00:00,  1.35it/s, lr0=0.00889, lr1=0.000889]\n",
      "100%|██████████| 117/117 [01:25<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4698 | Train Acc: 0.7790 | Val Loss: 0.5007 | Val Acc: 0.7803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.unfreeze(-1)\n",
    "one_cycle(model, criterion, optimizer, train_loader, val_loader, scheduler=scheduler, clip=0.25, device=device, lr_decrease=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [04:28<00:00,  1.02it/s, lr0=0.00667, lr1=0.000667]\n",
      "100%|██████████| 117/117 [02:26<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3806 | Train Acc: 0.8337 | Val Loss: 0.2880 | Val Acc: 0.8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.unfreeze(-2)\n",
    "one_cycle(model, criterion, optimizer, train_loader, val_loader, scheduler=scheduler, clip=0.25, device=device, lr_decrease=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [14:38<00:00,  3.22s/it, lr0=0.00445, lr1=0.000445]\n",
      "100%|██████████| 117/117 [03:03<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2986 | Train Acc: 0.8723 | Val Loss: 0.2323 | Val Acc: 0.9087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.unfreeze(-3)\n",
    "one_cycle(model, criterion, optimizer, train_loader, val_loader, scheduler=scheduler, clip=0.25, device=device, lr_decrease=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 255/273 [14:05<01:25,  4.77s/it, lr0=0.00238, lr1=0.000238]"
     ]
    }
   ],
   "source": [
    "model.unfreeze_all()\n",
    "one_cycle(model, criterion, optimizer, train_loader, val_loader, scheduler=scheduler, clip=0.25, device=device, lr_decrease=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_cycle(model, criterion, optimizer, train_loader, val_loader, scheduler=scheduler, clip=0.25, device=device, lr_decrease=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
